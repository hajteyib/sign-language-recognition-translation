# Sign Language Recognition and Translation (SLRT)

Deep Learning-based German Sign Language (DGS) to German text translation using Transformer architecture and MediaPipe landmarks.

## ğŸ“‹ Overview

This project implements an end-to-end pipeline for recognizing and translating German Sign Language (Deutsche GebÃ¤rdensprache - DGS) into written German text. Using the Phoenix-2014T dataset of weather broadcast sign language videos, the system extracts spatial landmarks via MediaPipe and translates them to text using a Transformer model.

### Key Features

- **Landmark-based approach**: Efficient sign language representation using MediaPipe (543 body landmarks)
- **Transformer architecture**: State-of-the-art sequence-to-sequence model
- **Data augmentation**: Spatial and temporal augmentations for improved generalization
- **Repetition penalty**: Eliminates infinite repetitions in generated text
- **Mac MPS support**: Optimized for Apple Silicon GPUs

## ğŸ¯ Results

| Configuration | Val Loss | Test Loss | BLEU | Training Time |
|--------------|----------|-----------|------|---------------|
| Baseline (1300 samples, 384-dim) | 4.62 | 3.46 | 23.1 | 2h58 |
| Large model (1300 samples, 512-dim) | 4.62 | 3.54 | 19.5 | 2h42 |
| **Final (2000 samples, 448-dim, aug)** | **4.42** | **3.32** | **27.4** | **2h04** |

**Improvements**: -4.3% validation loss, -4.0% test loss, +18.6% BLEU score, 60% reduction in mode collapse

## ğŸ—ï¸ Architecture

```
VidÃ©o LSA â†’ MediaPipe â†’ Landmarks (543 pts) â†’ Normalization
                                â†“
                        Spatial Embedding (448-dim)
                                â†“
                        Positional Encoding
                                â†“
                Transformer Encoder (4 layers, 8 heads)
                                â†“
                Transformer Decoder (4 layers, 8 heads)
                                â†“
                    Linear Layer (448 â†’ 2892)
                                â†“
                            Softmax
                                â†“
                        Texte Allemand
```

**Model Parameters**:
- d_model: 448
- Encoder/Decoder layers: 4 each
- Attention heads: 8
- Total parameters: 27.1M
- Vocabulary size: 2892

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ models/
â”‚   â””â”€â”€ transformer.py          # Transformer architecture
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ preprocessing.py        # MediaPipe landmark extraction
â”‚   â”œâ”€â”€ dataset.py             # Phoenix-2014T data loader
â”‚   â”œâ”€â”€ augmentation.py        # Data augmentation
â”‚   â”œâ”€â”€ vocabulary.py          # Vocabulary management
â”‚   â”œâ”€â”€ decoder.py             # Beam search + repetition penalty
â”‚   â””â”€â”€ experiment_tracker.py  # Training tracking
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract_landmarks.py   # Extract landmarks from videos
â”‚   â”œâ”€â”€ build_vocab.py         # Build vocabulary
â”‚   â”œâ”€â”€ compute_norm_stats.py  # Compute normalization stats
â”‚   â”œâ”€â”€ train.py               # Training script
â”‚   â””â”€â”€ evaluate.py            # Evaluation script
â”œâ”€â”€ run_extraction.sh          # Landmark extraction launcher
â”œâ”€â”€ run_training.sh            # Training launcher
â”œâ”€â”€ run_evaluation.sh          # Evaluation launcher
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- PyTorch 2.x (with MPS support for Mac)
- MediaPipe
- Phoenix-2014T dataset

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/sign-language-translation.git
cd sign-language-translation

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch torchvision torchaudio
pip install mediapipe opencv-python numpy pandas tqdm
```

### Dataset Setup

Download the Phoenix-2014T dataset and place it in the following structure:

```
data/
â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ phoenix14t.pami0.train.annotations_only.gzip
â”‚   â”œâ”€â”€ phoenix14t.pami0.dev.annotations_only.gzip
â”‚   â””â”€â”€ phoenix14t.pami0.test.annotations_only.gzip
â””â”€â”€ videos/
    â””â”€â”€ [video files]
```

### Pipeline Execution

**1. Extract Landmarks**
```bash
./run_extraction.sh --split train --max_samples 2000
./run_extraction.sh --split dev --max_samples 200
```

**2. Build Vocabulary**
```bash
python scripts/build_vocab.py
```

**3. Compute Normalization Statistics**
```bash
python scripts/compute_norm_stats.py --max_samples 2000
```

**4. Train Model**
```bash
./run_training.sh \
  --exp_name my_experiment \
  --d_model 448 \
  --nhead 8 \
  --num_encoder_layers 4 \
  --num_decoder_layers 4 \
  --batch_size 8 \
  --epochs 50 \
  --lr 3e-4 \
  --dropout 0.25
```

**5. Evaluate**
```bash
./run_evaluation.sh \
  --checkpoint checkpoints/my_experiment/models/best_model.pt \
  --split test
```

## ğŸ”¬ Key Components

### Data Augmentation

The pipeline includes several augmentation techniques:
- **Spatial**: Rotation (Â±10Â°), scaling (95-105%), translation (Â±3%)
- **Temporal**: Frame masking (10% probability)
- Applied with 50% probability during training

### Repetition Penalty

Custom n-gram repetition penalty prevents infinite loops:
```python
score_adjusted = score_original - Î» Ã— count(n-gram)
```
where Î» = 0.15 (tuned experimentally)

### Training Details

- **Optimizer**: AdamW (lr=3e-4, weight_decay=0.01)
- **Scheduler**: Linear warmup (800 steps) + cosine annealing
- **Loss**: Cross-entropy with label smoothing (0.15)
- **Early stopping**: Patience of 8 epochs
- **Gradient clipping**: Max norm 1.0

## ğŸ“Š Dataset

**Phoenix-2014T** (Weather broadcast sign language):
- Training: 2000 samples (28% of full dataset)
- Validation: 200 samples
- Test: 200 samples
- Vocabulary: 2892 unique German words
- Domain: Weather forecasts (inherently repetitive)

**Limitations**: The weather domain leads to naturally repetitive vocabulary and phrases, making complete elimination of mode collapse challenging.

## ğŸ“ˆ Experimental Results

### Ablation Studies

| Configuration | Impact |
|--------------|--------|
| +700 samples (1300â†’2000) | -2% val loss, +20% diversity |
| Data augmentation | -2% val loss, **+80% diversity** |
| 448-dim architecture | -0.5% val loss, +10% diversity |
| Weight decay 0.01 | -0.3% val loss |

**Key finding**: Data augmentation is the most impactful optimization for diversity.

## ğŸ› ï¸ Hardware

All experiments conducted on:
- **MacBook** with Apple Silicon (M1)
- **Backend**: PyTorch MPS (Metal Performance Shaders)




## ğŸ“š References

- [Phoenix-2014T Dataset](https://www-i6.informatik.rwth-aachen.de/~koller/RWTH-PHOENIX-2014-T/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [MediaPipe](https://google.github.io/mediapipe/)

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ‘¤ Author

**El haj Samitt Ebou**  
M2 Vision et Machine Intelligente  
UniversitÃ© Paris CitÃ©

---


